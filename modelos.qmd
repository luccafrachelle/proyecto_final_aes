---
title: "Modelos"
format:
  html:
    embed-resources: true
execute:
  echo: false
  warning: false
  message: false
---


```{r}
#| output: false
library(tidymodels)
library(tidyverse)
library(agua)
library(h2o)
library(readxl)
library(knitr)
library(rpart.plot)
library(yardstick)
library(tidymodels)
library(xgboost)
library(lightgbm)
library(baguette) 
library(bonsai)
library(gt)
library(vip)
library(doParallel)
library(knitr)
library(glmnet)
library(yardstick)
library(vip)
library(DALEXtra)


df <- read_csv("./models/data.csv")  %>%
  mutate(exito = factor(exito, levels = c(1, 0), labels = c("1", "0")))
train <- df %>% filter(dataset == "train")
test <- df %>% filter(dataset == "test")

```


# Modelos
Se proponenen 3 modelos distintos para el problema de clasificación de la variable exito. Uno logistico,  un random forest y  un LightGBM. La idea es usar la regresión logística para entender como influyen los distintos factores en la variable exito sin perder la interpretabilidad de los resultados. Los otros dos se prponen más con la idea de predecir la variable exito usando la información que se tiene en el conjunto de datos. Se eligen estos modelos uno produce menos overfitting(random forest) y otro produce menos error en la validación(LightGBM).


## Lasso

Se utiliza inicialmente un modelo de regresión logística como primer enfoque debido a su capacidad para proporcionar una interpretación clara y directa de las relaciones entre las variables predictoras y el resultado. 

```{r}
lasso = readRDS("./models/base_recipe_lasso_model.rds")
random_forest = readRDS("./models/base_recipe_random_forest_model.rds")
lightgbm = readRDS("./models/base_recipe_lightgbm_model.rds")
decision_tree = readRDS("./models/base_recipe_decision_tree_model.rds")

lasso_tune_results <- readRDS("./models/base_recipe_lasso_tune_results.rds")
random_forest_tune_results <- readRDS("./models/base_recipe_random_forest_tune_results.rds")
lightgbm_tune_results <- readRDS("./models/base_recipe_lightgbm_tune_results.rds")
decision_tree_tune_results <- readRDS("./models/base_recipe_decision_tree_tune_results.rds")

```

```{r}
lasso %>% 
  fit(train)  %>% 
  extract_fit_engine()  %>% 
  vi()  %>% 
  ggplot(aes(x = Importance, y = reorder(Variable, Importance), fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

```{r}
lasso %>% autoplot()
  ```

```{r}
lasso_tune_results %>% autoplot()
```

### Matriz de Confusion
```{r}
lasso_predictions_training <- predict(lasso, new_data = train, type = "class")%>% bind_cols(train) 

lasso_conf_matrix_training <- conf_mat(
  data = lasso_predictions_training,
  truth = exito,          
  estimate = .pred_class  
)

autoplot(lasso_conf_matrix_training, type = "heatmap") +
  labs(
    title = "Matriz de Confusión - LASSO (Entrenamiento)",
    x = "Clase Predicha",
    y = "Clase Real"
  ) +
  theme_minimal()

```

## Arbol de Decisión


```{r} 
decision_tree %>% extract_fit_engine() %>% rpart.plot(main = "Árbol de Decisión Ajustado")
```

```{r}
autoplot(decision_tree_tune_results)
```

```{r}
tree_predictions <- predict(decision_tree, new_data = train, type = "class") %>%
  bind_cols(train) 

tree_conf_matrix <- conf_mat(
  data = tree_predictions,
  truth = exito,          
  estimate = .pred_class  
)

autoplot(tree_conf_matrix, type = "heatmap") +
  labs(
    title = "Matriz de Confusión - Árbol de Decisión",
    x = "Clase Predicha",
    y = "Clase Real"
  ) +
  theme_minimal()

```

## Random Forest


### Visualización de la evolución de las métricas según los parámetros

```{r}
autoplot(random_forest_tune_results) +
  labs(
    title = "Evolución de las Métricas - Random Forest",
    x = "Hiperparámetros",
    y = "Métrica (ROC AUC, Accuracy, etc.)"
  ) +
  theme_minimal()
```


### Matriz de Confusion
```{r}
rf_predictions_train <- predict(random_forest, train) %>%
  bind_cols(train)

rf_conf_matrix_train <- conf_mat(
  data = rf_predictions_train,
  truth = exito,
  estimate = .pred_class
)

autoplot(rf_conf_matrix_train, type = "heatmap") +
  labs(
    title = "Matriz de Confusión - Random Forest (Entrenamiento)",
    x = "Clase Predicha",
    y = "Clase Real"
  ) +
  theme_minimal()
```

### Importancia de las variables

```{r}
random_forest %>%
  extract_fit_engine() %>%
  vip::vip(geom = "col", aesthetics = list(fill = "steelblue")) +
  labs(
    title = "Importancia de las Variables - Random Forest",
    x = "Importancia Relativa",
    y = "Variable"
  ) +
  theme_minimal()
```


## LightGBM

### Visualización de la evolución de las métricas según los parámetros
```{r}
autoplot(lightgbm_tune_results) +
  labs(
    title = "Evolución de las Métricas - LightGBM",
    x = "Hiperparámetros",
    y = "Métrica (ROC AUC, Accuracy, etc.)"
  ) +
  theme_minimal()
```

### Matriz de Confusion
```{r}
lgbm_predictions_train <- predict(lightgbm, train) %>%
  bind_cols(train)

lgbm_conf_matrix_train <- conf_mat(
  data = lgbm_predictions_train,
  truth = exito,
  estimate = .pred_class)

autoplot(lgbm_conf_matrix_train, type = "heatmap") +
  labs(
    title = "Matriz de Confusión - LightGBM (Entrenamiento)",
    x = "Clase Predicha",
    y = "Clase Real"
  ) +
  theme_minimal()
```


```{r}
lightgbm %>%
  extract_fit_engine() %>%
  vip::vip(geom = "col", aesthetics = list(fill = "steelblue")) +
  labs(
    title = "Importancia de las Variables - LightGBM",
    x = "Importancia Relativa",
    y = "Variable"
  ) +
  theme_minimal()
```


```{r}
lasso_predictions_training <- predict(lasso, train) %>%
  bind_cols(train)

tree_predictions <- predict(decision_tree, train) %>%
  bind_cols(train)

rf_predictions <- predict(random_forest, train) %>%
  bind_cols(train)

lgbm_predictions_train <- predict(lightgbm, train) %>% 
  bind_cols(train)
```



## Evalución de los modelos
```{r}
final_metrics <- metric_set(accuracy, sens, yardstick::spec, roc_auc)

calculate_model_metrics <- function(fit, split_data) {
  preds <- predict(fit, new_data = split_data, type = "class") %>%
    bind_cols(predict(fit, new_data = split_data, type = "prob")) %>%
    bind_cols(truth = split_data$exito)
  
  metrics_df <- final_metrics(
    data = preds, 
    truth = truth,
    estimate = .pred_class,
    .pred_1
  )
  
  return(metrics_df)
}

lasso_metrics <- calculate_model_metrics(lasso, train)
tree_metrics <- calculate_model_metrics(decision_tree, train)
rf_metrics <- calculate_model_metrics(random_forest, train)
lgbm_metrics <- calculate_model_metrics(lightgbm, train)

model_performance <- bind_rows(
  Lasso = lasso_metrics,
  `Decision Tree` = tree_metrics,
  `Random Forest` = rf_metrics,
  LightGBM = lgbm_metrics,
  .id = "Model"
) %>%
  select(Model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate)

model_performance %>%
  gt() %>%
  fmt_number(
    columns = c(accuracy, sens, spec, roc_auc),
    decimals = 3
  ) %>%
  cols_label(
    accuracy = "Accuracy",
    sens = "Sensitivity",
    spec = "Specificity",
    roc_auc = "ROC AUC"
  ) %>%
  tab_header(
    title = "Model Performance Metrics"
  )

```


## Datos de testeo

```{r}
lasso_metrics <- calculate_model_metrics(lasso, test)
tree_metrics <- calculate_model_metrics(decision_tree, test)
rf_metrics <- calculate_model_metrics(random_forest, test)
lgbm_metrics <- calculate_model_metrics(lightgbm, test)

model_performance <- bind_rows(
  Lasso = lasso_metrics,
  `Decision Tree` = tree_metrics,
  `Random Forest` = rf_metrics,
  LightGBM = lgbm_metrics,
  .id = "Model"
) %>%
  select(Model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate)

model_performance %>%
  gt() %>%
  fmt_number(
    columns = c(accuracy, sens, spec, roc_auc),
    decimals = 3
  ) %>%
  cols_label(
    accuracy = "Accuracy",
    sens = "Sensitivity",
    spec = "Specificity",
    roc_auc = "ROC AUC"
  ) %>%
  tab_header(
    title = "Model Performance Metrics"
  )
```

## Distriubución de posiciones iniciales
```{r}
df %>%
  ggplot(aes(x = x_inicio, y = y_inicio)) +
  geom_bin2d(bins = 15) +
  scale_fill_gradient(low = "blue", high = "red") + 
geom_segment(aes(x=0,xend=120,y=0,yend=0))+
geom_segment(aes(x=0,xend=120,y=80,yend=80))+
geom_segment(aes(x=0,xend=0,y=0,yend=80))+
geom_segment(aes(x=120,xend=120,y=0,yend=80))+
geom_segment(aes(x=60,xend=60,y=0,yend=80))+
geom_segment(aes(x=0,xend=120,y=62,yend=62))+
geom_segment(aes(x=0,xend=120,y=18,yend=18))+
geom_segment(aes(x=0,xend=120,y=30,yend=30))+
geom_segment(aes(x=0,xend=120,y=50,yend=50))+
geom_segment(aes(x=18,xend=18,y=0,yend=80))+
geom_segment(aes(x=39,xend=39,y=0,yend=80))+
geom_segment(aes(x=102,xend=102,y=0,yend=80))+
geom_segment(aes(x=81,xend=81,y=0,yend=80)) +
theme(legend.position = "bottom") + facet_wrap(~exito) + coord_flip() + 
  labs(title = "Posiciones de inicio segun exito", x = "Coordenada X", y = "Coordenada Y")
```

## Ver donde se equivoca el Random Forest
```{r}
temp = predict(random_forest, new_data = train, type = "class") %>%
  bind_cols(predict(random_forest, new_data = train, type = "prob")) %>%
  bind_cols(train)

temp %>% filter(exito != .pred_class) %>%
  ggplot(aes(x = x_inicio, y = y_inicio)) +
  geom_bin2d(bins = 15) +
  scale_fill_gradient(low = "blue", high = "red") + 
geom_segment(aes(x=0,xend=120,y=0,yend=0))+
geom_segment(aes(x=0,xend=120,y=80,yend=80))+
geom_segment(aes(x=0,xend=0,y=0,yend=80))+
geom_segment(aes(x=120,xend=120,y=0,yend=80))+
geom_segment(aes(x=60,xend=60,y=0,yend=80))+
geom_segment(aes(x=0,xend=120,y=62,yend=62))+
geom_segment(aes(x=0,xend=120,y=18,yend=18))+
geom_segment(aes(x=0,xend=120,y=30,yend=30))+
geom_segment(aes(x=0,xend=120,y=50,yend=50))+
geom_segment(aes(x=18,xend=18,y=0,yend=80))+
geom_segment(aes(x=39,xend=39,y=0,yend=80))+
geom_segment(aes(x=102,xend=102,y=0,yend=80))+
geom_segment(aes(x=81,xend=81,y=0,yend=80)) +
theme(legend.position = "bottom") +
  labs(title = "Errores en los datos de entrenamiento") + coord_flip() 
```

## Errores en el testeo
```{r}
temp = predict(random_forest, new_data = test, type = "class") %>%
  bind_cols(predict(random_forest, new_data = test, type = "prob")) %>%
  bind_cols(test)

temp %>% filter(exito != .pred_class) %>%
  ggplot(aes(x = x_inicio, y = y_inicio)) +
  geom_bin2d(bins = 15) +
  scale_fill_gradient(low = "blue", high = "red") + 
geom_segment(aes(x=0,xend=120,y=0,yend=0))+
geom_segment(aes(x=0,xend=120,y=80,yend=80))+
geom_segment(aes(x=0,xend=0,y=0,yend=80))+
geom_segment(aes(x=120,xend=120,y=0,yend=80))+
geom_segment(aes(x=60,xend=60,y=0,yend=80))+
geom_segment(aes(x=0,xend=120,y=62,yend=62))+
geom_segment(aes(x=0,xend=120,y=18,yend=18))+
geom_segment(aes(x=0,xend=120,y=30,yend=30))+
geom_segment(aes(x=0,xend=120,y=50,yend=50))+
geom_segment(aes(x=18,xend=18,y=0,yend=80))+
geom_segment(aes(x=39,xend=39,y=0,yend=80))+
geom_segment(aes(x=102,xend=102,y=0,yend=80))+
geom_segment(aes(x=81,xend=81,y=0,yend=80)) +
theme(legend.position = "bottom") + facet_wrap(~exito) + coord_flip() +
  labs(title = "Errores en el testeo", x = "Coordenada X", y = "Coordenada Y")
```


## Interpretabilidad del RandomForest

```{r}
train$exito_num <- as.numeric(train$exito) - 1

explainer_rf <- explain_tidymodels(
  model = random_forest,
  data = train %>% select(-exito, -exito_num), 
  y = train$exito_num,
  label = "Random Forest",
  verbose = FALSE
)
vip_rf <- readRDS("./models/vip_rf.rds")
```


```{r}
set.seed(1805)
pdp_neventos <- model_profile(explainer_rf, N = 9000, variables = "n_eventos")

plot(pdp_neventos) + geom_rug()
```


```{r}
set.seed(1805)
pdp_vert <- model_profile(explainer_rf, N = 9000, variables = "vert_tot")

plot(pdp_vert) + geom_rug()
```

```{r}
set.seed(1805)
pdp_vert <- model_profile(explainer_rf, N = 9000, variables = "dist.promP")

plot(pdp_vert) + geom_rug()
```
